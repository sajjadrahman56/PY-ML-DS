# -*- coding: utf-8 -*-
"""Pytorch-beginer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fq3gx2u_3gvT3pKUe4Hj9jrgTgpecp1e
"""

# Pytorch
#
# Qick Iteration
# Seamless graph model transition with TorchScript
# production ready with TorchServe
#

!pip install torch torchvision -U

import torch

print(torch.__version__)

import torch as tor

t0 = tor.tensor(1000)
t1 = tor.tensor([1,2,3,5,6,7])
t2 = tor.tensor([[1,2,3],[7,8,5]])
print(t0)
print(t1)
print(t2)

# change the dimension of tensor is
# Tensor.view(nrows,ncols)
#
#

from torch.autograd import Variable

x = Variable(torch.ones(2,2),requires_grad=True)
print(x)

y = x + 2
y

print(y.grad_fn)

z = y * y * 3
out = z.mean()

print(z,out)

"""### Gradient Calculation"""

import torch as tor
x = tor.rand(3,requires_grad=True)
print(x)

y = x + 2
print(y)

z = y * y * 2
z

#z = z.mean()
#z

# grad can be implicitly created only for scalar outputs
# z mean give us one value
#
# vector jocovian product learn

v = torch.tensor([0.1,1.0,0.001],dtype = torch.float32)

z.backward(v)

print(x.grad)





"""# NB
whenever _ in the end of fuctions which means its update the variable
"""

# detch
import torch
x = torch.randn(3,requires_grad=True)
print(x)

y = x.detach()
y
# the output of Y is same as like the output of X but the missing is requires_grad=True
#
# which is a boolean value that determines whether or not gradients should be computed for the tensor.
# Gradients are used in backpropagation to update the weights of a neural network during training.
# By default, tensors in PyTorch have .requires_grad set to True.

"""## Forward and Backward propagation with Torch"""

# forward pass
import torch as tr

x = tr.tensor(1.0)
y = tr.tensor(2.0)

w = tr.tensor(1.0,requires_grad=True)

y_hat = w*x
loss = (y_hat - y)**2
print(loss)

#  backword pass
loss.backward()
print(w.grad)

### update weights
# next forward and backwards
torch.device('cuda:0')

x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])

x.stride()

x.t().stride()

# https://pytorch.org/docs/stable/nn.html#containers
#

"""### Manualy implemented
- numpy use
"""

import numpy as np
# f = w * x
# f = 2 * x
x = np.array([1,2,3,4],dtype=np.float32)
y = np.array([2,4,6,8],dtype=np.float32)

w = 0.0
# model prediction
def forward(x):
  return w * x
# loss
def loss(y,y_predict):
  return ((y_predict - y)**2).mean()
# Gradient
# mse = 1/N * ( w*x - y)**2
# dz/dw = 1/N 2x(w*x-y)

def gradient(x,y,y_predict):
  return np.dot(2*x,y_predict-y).mean()

print(f'prediction before traning f(5) = {forward(5):.3f}')

#Training
learning_rate = 0.01
n_iters = 20

for epoch in range(n_iters):
  y_pred  = forward(x)

  l= loss(y,y_pred)

  dw = gradient(x,y,y_pred)

  w -= learning_rate*dw

  if epoch%2 ==0:
    print(f'epoch = {epoch+1} w = {w:.3f} and loss = {l:.8f}')
print(f'prediction after traning f(5) = {forward(5):.3f}')

"""### Torch use"""

import torch as tr
# f = w * x
# f = 2 * x
x = tr.tensor([1,2,3,4],dtype=tr.float32)
y = tr.tensor([2,4,6,8],dtype=tr.float32)

w = tr.tensor(0.0,dtype=tr.float32,requires_grad=True)

# model prediction
def forward(x):
  return w * x

# loss
def loss(y,y_predict):
  return ((y_predict - y)**2).mean()

# Gradient
# mse = 1/N * ( w*x - y)**2
# dz/dw = 1/N 2x(w*x-y)

print(f'prediction before traning f(5) = {forward(5):.3f}')

#Training
learning_rate = 0.01
n_iters = 100

for epoch in range(n_iters):
  y_pred  = forward(x)

  l= loss(y,y_pred)
  # backword
  l.backward()
  with tr.no_grad():
      w -= learning_rate*w.grad
  w.grad.zero_()

  if epoch%10 ==0:
    print(f'epoch = {epoch+1} w = {w:.3f} and loss = {l:.8f}')
print(f'prediction after traning f(5) = {forward(5):.3f}')

import torch.nn as nn
m = nn.Conv1d(16, 33, 3, stride=2)
input = torch.randn(20, 16, 50)
output = m(input)

output.shape

